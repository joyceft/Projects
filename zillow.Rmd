---
title: "HW1_Tianyi Fang"
author: "Tianyi Fang"
date: "September 25, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install library
library(data.table)
library(ggplot2)
library(dplyr)
library(tidyr)#filter/mutate/select
library(lubridate)#date
library(corrplot)
library(plyr)#rename
library(lattice)

```
##load and prepare the data
####1.Load data
We can see there are parcelid, logerror, transaction date in train_2016, 57 features of 298w properties. Most of the features are numeric, some are characters, notice some features contain lots of NA.
```{r}
train <- read.csv("train_2016_v2.csv", header = TRUE, stringsAsFactors = FALSE)
property <- read.csv("rename_property.csv", stringsAsFactors = FALSE)
dim(train)
dim(property)
str(property)
```
####2. Rename feature names
Since the original feature names are quite long and confusing, rename them in a proper way. 
```{r}
property <- rename(property, c("parcelid=id_parcel",
                   'threequarterbathnbr'=	'cnt_0.75_bath',
'bathroomcnt'=	'cnt_bathr',
'bedroomcnt'=	'cnt_bedr',
'calculatedbathnbr'=	'cnt_cal_bathr',
'fireplacecnt'=	'cnt_fireplace',
'fullbathcnt'=	'cnt_fullbath',
'garagecarcnt'=	'cnt_garage',
'poolcnt'=	'cnt_pool',
'roomcnt'=	'cnt_room',
'numberofstories'=	'cnt_stories',
'unitcnt'=	'cnt_unit',
'propertycountylandusecode'=	'code_landuse',
'propertyzoningdesc'=	'descb_property',
'fips'=	'fips',
'fireplaceflag'=	'flag_fireplace',
'hashottuborspa'=	'flag_tub',
'parcelid'=	'id_parcel',
'latitude'=	'latitude',
'longitude'=	'longtitude',
'buildingqualitytypeid'=	'quality',
'regionidcity'=	'region_city',
'regionidcounty'=	'region_county',
'regionidneighborhood'=	'region_neighbor',
'regionidzip'=	'region_zip',
'finishedsquarefeet6'=	'sqrt_base',
'basementsqft'=	'sqrt_basement',
'calculatedfinishedsquarefeet'=	'sqrt_cal_area',
'finishedsquarefeet12'=	'sqrt_finished_liv',
'finishedfloor1squarefeet'=	'sqrt_floor1',
'garagetotalsqft'=	'sqrt_garage',
'lotsizesquarefeet'=	'sqrt_lot',
'yardbuildingsqft17'=	'sqrt_patio',
'finishedsquarefeet13'=	'sqrt_perimeter_liv',
'poolsizesum'=	'sqrt_pool',
'yardbuildingsqft26'=	'sqrt_shed',
'finishedsquarefeet15'=	'sqrt_total',
'finishedsquarefeet50'=	'sqrt_unknown',
'structuretaxvaluedollarcnt'=	'tax_building',
'taxdelinquencyflag'=	'tax_delinq',
'taxdelinquencyyear'=	'tax_delinq_year',
'landtaxvaluedollarcnt'=	'tax_land',
'taxamount'=	'tax_property',
'taxvaluedollarcnt'=	'tax_total',
'assessmentyear'=	'tax_year',
'airconditioningtypeid'=	'type_air',
'architecturalstyletypeid'=	'type_archit',
'decktypeid'=	'type_deck',
'buildingclasstypeid'=	'type_frame',
'heatingorsystemtypeid'=	'type_heating',
'propertylandusetypeid'=	'type_landuse',
'typeconstructiontypeid'=	'type_material',
'storytypeid'=	'type_story',
'yearbuilt'=	'year_built',
'rawcensustractandblock'='id_rawcensus_block',
'censustractandblock'='id_census-block'))

train <- rename(train, c("parcelid"="id_parcel"))
property <- property[, -1]
write.csv(property, "rename_property_1.csv")
colnames(property)

```
####3. First glance at the raw data and merge
```{r}
summary(train$logerror)
#check how many unique parcelid in both files
length(unique(train$id_parcel))
length(unique(property$id_parcel))

#find common parcelid, so that to get some info for logerror vs features
property_1 <- subset(property, id_parcel %in% train$id_parcel)
pt <- merge(train, property_1, by = "id_parcel", all.x = TRUE)#left inner join
pt <- pt[,-1]
write.csv(pt_remain, "train_property.csv")
```
take a look at the data, there are 58 properties(including parcelid), 2.98 million observations in properties_2016, and the train file, it has 90275 observations, with 3 features: id_parcel, logerror and transactiondate.If we want to find the relationship between logerror and those features, we need to combine 2 files in one table, based on those parcels show in train dataset(left join).\\

####4. Distribution of logerror
```{r}
pt<- read.csv("train_property.csv", header = TRUE, stringsAsFactors = FALSE)
pt %>%
  ggplot(aes(x=logerror))+
  geom_histogram(bins=600, fill = "darkblue")+ylab("Count") + coord_cartesian(x=c(-0.5, 0.5))+ ggtitle("Histogram of logerror")

```

####5.Dealing with NA
```{r}
#check # of NA in each feature
length(which(is.na(pt$type_archit)))
#use n_distinct(pt$architecturalstyletypeid) in dplyr is faster and more concise than length(unique())

num.NA.pt <- sort(colSums(sapply(pt, is.na)))
#check every feature, is there missing value
#sort(sapply(pt, function(x){sum(is.na(x))}), decreasing= TRUE)
na.df <- data.frame(num.NA.pt)
na.df$names <- rownames(na.df)

na.df%>%
  ggplot(aes(x=reorder(names, -num.NA.pt), y=num.NA.pt)) +
  geom_bar(stat="identity", fill="darkblue")+ labs(title="Number of NA of features", x="features", y="Amount of NA")+ theme(axis.text.x=element_text(angle=80, hjust = 1))
        
```
we can see that nearly 18 features have no NA, features such as type_frame, sqrt_perimeter_liv, type_story, sqrt_basement, sqrt_shed, type_archit, type_material, sqrt_base, have nearly 90000 NA, which means these features might contain little information and might be helpless for our analysis. So when analysis the relationship between logerror and features, those features with majority NA will less likely be considered. 
```{r}
pt_remain <- pt[,remain.col]
str(pt_remain)
#rigion
#cntnathroom, cntbathroom are totally the same, igore one. 
```
We want to know the percentage of NA in pt. It's nearly 41%. So we have to decide  how to treat with features with majority NA. We may want features with NA less than 20%. There are 31(including parcelid) left. 

```{r}
na.percent <- sum(is.na(pt))/(nrow(pt)*ncol(pt))
remain.col <- names(num.NA.pt)[which(num.NA.pt <= 0.2 * dim(pt)[1])]
majorna.col <- names(num.NA.pt)[which(num.NA.pt> 0.2 * dim(pt)[1])]
pt_remain %>% mutate(abs_logerror = abs(logerror))%>%
    ggplot(aes(x=abs_logerror))+
    geom_histogram(bins=600) + labs(y="Count", title="Histogram of abs(logerror)")+coord_cartesian(x=c(0, 0.5))
```
In these case, since the # of features are large enought, so we only focus on features with less than 20% of NA. But take a glance at those dropped features, cnt_garage, cnt_0.75_bath, cnt_fireplace, cnt_pool, cnt_stories, sqrt_pool, sqrt_shed, might indict that those properties have no such features, like pool, 3/4 bathroom, so NA may means 0. We can set them as binary factors:0=none, 1=have to check whether having these features will impact our logerror. 
```{r}
major_na <- pt[,majorna.col]
major_na <- cbind(major_na, pt$logerror)
check1 <- c("cnt_garage", 'cnt_0.75_bath', 'cnt_fireplace', 'cnt_pool', 'cnt_stories', 'sqrt_pool', "sqrt_shed")
name1 <- which(colnames(major_na) %in% check1)
checkna1 <- major_na[, name1]
checkna1[checkna1 != "NA"] <-1
checkna1[is.na(checkna1)] <- 0
checkna1 <- cbind(checkna1, pt$logerror)
checkna1 %>% group_by(.dots = c("cnt_garage", 'cnt_0.75_bath', 'cnt_fireplace', 'cnt_pool', 'cnt_stories', 'sqrt_pool', "sqrt_shed")) %>% summarize(x=mean(pt$logerror))
xyplot(pt$logerror ~cnt_garage, data = checkna1)
#get the mean of logerror for x=0/x=1, then compare which mean.logerror is larger. for all xi in checkna1.

```
for "quality" "type_heating" do the same based on their groups. I will illustrate more about how to deal with NA data.

#### How logerror change with month?
Since the transaction year is all 2016, we only care about how logerror change from each month.
```{r}
boxplot(logerror ~ transactiondate, data = pt_remain)

pt_remain[, "transactiondate"] <- as.character(pt_remain$transactiondate)
#extract month
pt_remain$month <- sapply(strsplit(pt_remain$transactiondate, "-"), function(x) x[2])
#boxplot of each month
pt_remain %>% subset(abs(logerror)<0.15) %>%
  ggplot(aes(month, logerror, color=month))+
  geom_boxplot()+labs(title="Boxplot of logerror in each month")

err_month <- by(pt_remain, pt_remain$month, function(x){
  return(mean(x$logerror))})
plot(err_month, type="l", xlab="Month", ylab="Logerror", main = "Logerror of month", col="darkblue")
abs_err_month <- by(pt_remain, pt_remain$month, function(x){
  return(mean(abs(x$logerror)))})
plot(abs_err_month, type="l", xlab="Month", ylab="Abs_Logerror", main = "Abs_Logerror of month", col="darkblue")

```
Set x's range as -0.5~0.5, because the majority part of logerror are among this region. 
From the outcomes we can see: logerror is log(zestimate)-log(actualprice), so the absolute logerror means how close Zestimate is close to actual price.
From the plots of "logerror vs month" and "abs(logerror) vs month", we can see that both mean and abs(mean) go down from Feb to April, then increase from July to September, go down to November and largely increase during December. That is, at the beginning and ending of 2016, the Zestimate error is larger, while during the middle of 2016, the Zestimate seems to be close to the real transaction prices.

```{r}
pt_remain_abs$month <- factor(pt_remain_abs$month)

pt_remain_abs %>% select(month,abs_logerror)%>%
  group_by(month) %>% summarize(mean_abs_logerror = mean(abs_logerror)) %>%
  ggplot(aes(x=month, y= mean_abs_logerror)) + 
  geom_line(linetype = 2, color = "darkblue") +
  geom_point(size = 3, color = "darkblue") + ggtitle("Abs(logerror) vs month")
pt_remain %>% group_by(month) %>% summarize(mean_logerror = mean(logerror)) %>%
  ggplot(aes(x=month, y= mean_logerror)) + 
  geom_line(linetype = 2, color = "darkblue") +
  geom_point(size = 3, color = "darkblue") + ggtitle("logerror vs month")
  
```
####Drop useless features
```{r}
unique1.r<- as.data.frame(summary(apply(pt_remain, 2, unique)))[1:32,]
unique1.r <- arrange(unique1.r, desc(Freq))
unique1.r%>%
  ggplot(aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill="darkblue")+ labs(title="Number of Unique features", x="features", y="Amount of unique value")+ theme(axis.text.x=element_text(angle=80, hjust = 1))
pt_remain <- pt_remain[,-17]
```

From the count of unqiue values of each feature, we find that even though there is no missing value of id_parcel, but the unqiue values are still less than total number. That is, there must be dupicate ids. Also, tax_year has noly one value, which can be dropped. Also, flag_tub, flag_fireplace and tax_delinq only have 2 values, which means they might indicate boolean value. Fips and Region_county have 3 categories.region_zip, id_census.block, region_city, year_built, code_landuse, cnt_bathr, cnt_cal_bathr, cnt_bedr, cnt_room, cnt_fullbath, type_landuse and month have less values, which might also be treated as categorical features.
Before analysis each feature, let's take a look at their correlation.
1.categorical features check
```{r}
cor.total <- cor(pt_remain[, c("logerror", "cnt_bathr", "cnt_bedr","fips", "latitude", "longtitude", "type_landuse",  "id_rawcensus_block", "region_county", "cnt_room","tax_total", "tax_land","tax_property", "region_zip","tax_building", "id_census.block","sqrt_cal_area","year_built","cnt_cal_bathr", "cnt_fullbath", "region_city", "sqrt_finished_liv", "sqrt_lot")], use="pairwise.complete.obs")

cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
    diag(p.mat) <- 0
    diag(lowCI.mat) <- diag(uppCI.mat) <- 1
    for(i in 1:(n-1)){
        for(j in (i+1):n){
            tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
            p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
            lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
            uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
        }
    }
    return(list(p.mat, lowCI.mat, uppCI.mat))
}
total.cor1 <- cor.mtest(cor.total, 0.95)
total.cor2 <- cor.mtest(cor.total, 0.99)
```

```{r}
#corrplot(cor, method = "ellipse", t1.cex=1, type= "upper")
#corrplot(cor, p.mat= res1[[1]], sig.level = 0.2, method = "circle")
#corrplot(cor, p.mat = res2[[1]], sig.level = 0.2, method = "ellipse")
#give a x to those not significant coefficient
c_95 <- corrplot(total.cor1, p.mat=res1[[1]], insig = "blank", type = "upper")
c_99 <- corrplot(cor, p.mat=res2[[1]], insig = "blank", type = "upper")
#correlations <- cor(pt_remain_date)
c_99
dev.copy(png, file = "cor1.png")
dev.off()
#corrplot.mixed(pt_remain_date)
```
From the correlation plot, we can find there are large relationship between: 
\\
cnt_bathr&cnt_bedr, tax_total, tax_land, tax_property, tax_building, sqrt_cal_area, cnt_cal_bathr, cnt_fullbath, sqrt_finished_liv
fips&id_rawcensus_block,region_county, tax_total, id_census_block
tax_total&tax_property&tax_building&sqrt_area&cnt_bath&sqrt_finished_liv
sqrt_cal_area&cnt_cal_bathr&cnt_fullbath
\\
those features are highly correlated with each other. Some make sense, for example, in the tax_group, the more tax_total is, the more land and property values are, and of couse, the tax value of one house depends on its area, location, house age and so on. Rooms and area are also related groups. While none of these features seems to be highly related to logerror.
\\
But neither of them are highly correlated with logerror
#### few levels features
```{r}
summary(pt_remain)
summary(pt_remain$cnt_bathr)
summary(pt_remain$cnt_cal_bathr)
sum(pt_remain$cnt_bathr %in% pt_remain$cnt_cal_bathr)
summary(pt_remain$cnt_bedr)
summary(pt_remain$cnt_room)
with(pt_remain, plot(cnt_bathr+cnt_bedr, cnt_room))
```
make some hypothesis test of cnt_room, maybe there is something wrong with data(mean of the number of total room is less than the sum of bedroom and bathroom)
#Assume logerror do not show significant difference between group1(room<bed+bath), group2(room>bed+bath). According to the result, p<0.05, reject H0. Logerror do differ a lot between 2 groups, but since they are highly correlated to each other, we may only choose one of them into our models, especially for bathroom
```{r}
#t.test(x,y)do t test for x, y, check the difference between their means
#anova: t.test(X~A, data=x_data), A are factors(like model), using F-value
#with(pt_remain, cnt_bathr+cnt_bedr<cnt_room)<<return T/F for each cnt_room
with(pt_remain, t.test(logerror~(cnt_room < cnt_bathr + cnt_bedr)))
with(pt_remain, t.test(abs(logerror)~(cnt_room < cnt_bathr + cnt_bedr)))
#the above t-test tests whether logerror has significant difference in group1(room<bed+bath), group2(room>bed+bath)
#reject H0: abs(logerror) in 2 groups differ a lot.
with(pt_remain, plot(logerror~(cnt_bathr+cnt_bedr<cnt_room)))

```
The assessmentyear has only 1 value as 2015. So we can ignore it.\\
Then we take a look at the categorical features(with few levels) about fips.
There are 3 levels, "6037"(LA) contains 58574, "6059"(OrangeCounty) contains 24505, and "6111"(VenturaCounty) contains 7146. Check whether logerror preform differently in different fips.

LA has 0.012 logerror vs 0.0112 exclude_logerror, p=0475<0.5; OC has 0.012 vs 0.0104, p-value = 0.18>0.05; VC has 0.011 vs 0.017, p-value=0.001<0.05; So both LA and VC cause great fluctuation of logerror.\\
VC has the largest range of logerror, the reason may be it contains smaller amount of obs(sparse data), so for other features with sparse data, we can expect large logerror. 
```{r}
table(pt_remain$fips)# check by groups
pt_remain$fips <- as.character(pt_remain$fips)#since fips are int, change them into chr so can be used as factor
xyplot(logerror~fips, data= pt_remain)
xyplot(abs(logerror)~fips, data = pt_remain)
with(pt_remain, t.test(logerror~(fips=="6037")))
with(pt_remain, t.test(logerror~(fips=="6059")))
with(pt_remain, t.test(logerror~(fips=="6111")))

with(pt_remain, t.test(abs(logerror)~(fips=="6037")))
with(pt_remain, t.test(abs(logerror)~(fips=="6059")))
with(pt_remain, t.test(abs(logerror)~(fips=="6111")))
with(pt_remain, anova(lm(logerror~fips)))
with(pt_remain, anova(lm(abs(logerror)~fips)))
```
For categorical variables with many levels, we can set regionidcity 
\\
For example, region_zip has 389 levels. city has 178, county has 3.
```{r}
#how many unique value for region?
length(unique(pt_remain$region_zip))#389
length(unique(pt_remain$region_city))#178
length(unique(pt_remain$region_county))#3
#how many NA?
sum(is.na(pt_remain$region_zip))#35
sum(is.na(pt_remain$region_city))#1083
sum(is.na(pt_remain$region_county))#0
#maybe we want to replace NA with its nearby value
error.zip <- pt_remain %>% select(region_zip, logerror, id_parcel )%>%group_by(region_zip)%>%summarize(a_logerror = mean(logerror), meanid=mean(id_parcel))
plot(density(error.zip))

#group by zip, plot frequency of logerror in each zip group
#how they disperse
xyplot(logerror~region_county, data= pt_remain)
xyplot(logerror~region_city, data= pt_remain)
xyplot(logerror~region_zip, data= pt_remain)
table(pt_remain$region_zip)
table(pt_remain$region_city)
table(pt_remain$region_county)
```
month
####Check correlation

```{r}
pt_remain_abs %>% ggplot(aes(x=year_built)) + geom_line(stat="density")
#yearbuilt
pt_remain %>% group_by(year_built) %>%
  summarize(mean_abs_logerror = mean(abs(logerror)), n()) %>%
  ggplot(aes(x=year_built, y=mean_abs_logerror))+
  geom_smooth(color = "grey40")+
  geom_point(color="red")
#bathroomcnt
pt_remain_abs %>% group_by(cnt_bathr) %>%
  summarize(mean_abs_logerror = mean(abs(logerror)), n()) %>%
  ggplot(aes(x=cnt_bathr, y=mean_abs_logerror))+
  geom_smooth(color = "grey40")+
  geom_point(color="red")
#bedroomcnt
pt_remain_abs %>% group_by(bedroomcnt) %>%
  summarize(mean_abs_logerror = mean(abs(logerror)), n()) %>%
  ggplot(aes(x=bedroomcnt, y=mean_abs_logerror))+
  geom_smooth(color = "grey40")+
  geom_point(color="red")
#roomcnt
pt_remain_abs %>% group_by(roomcnt) %>%
  summarize(mean_abs_logerror = mean(abs(logerror)), n()) %>%
  ggplot(aes(x=roomcnt, y=mean_abs_logerror))+
  geom_smooth(color = "grey40")+
  geom_point(color="red")
#fips
pt_remain %>% group_by(fips) %>%
  summarize(mean_abs_logerror = mean(abs(logerror)), n()) %>%
  ggplot(aes(x=fips, y=mean_abs_logerror))+
  geom_smooth(color = "grey40")+
  geom_point(color="red")
##continuous

#tax_value
pt_remain_abs %>% 
  ggplot(aes(x=taxvaluedollarcnt, y=abs_logerror))+
  geom_smooth(color = "grey40")
#tax_amount
pt_remain_abs %>%
  ggplot(aes(x=taxamount, y=abs_logerror))+
  geom_smooth(color = "grey40")
#structure value
pt_remain_abs %>%
  ggplot(aes(x=structuretaxvaluedollarcnt, y=abs_logerror))+
  geom_smooth(color = "grey40")
#square_finished_feet
pt_remain_abs %>%
  ggplot(aes(x=calculatedfinishedsquarefeet, y=abs_logerror))+
  geom_smooth(color = "grey40")
#sqaure_lot
pt_remain_abs %>%
  ggplot(aes(x=lotsizesquarefeet, y=abs_logerror))+
  geom_smooth(color = "grey40")

```
From the plot, we can see the logerror is reduced as for recently built houses, it is possible that Zestimate have less power to those old houses because they have more influential/unpredictable factors will affect the sale price.
it seems like only lotsize has a positive relationship with logerror, which means the Zestimate may get more away from the real saleprice as lotsize goes up.For the taxvalue, taxamount, it seems Zestimate did worse from lower to middle part, then from middle part, its perform goes better. 
####handling with numeric NA with MICE
```{r}
md.pattern(pt_remain)
marginplot(pt_remain[c(1,2)])
#need no NA
agr_plot <- aggr(pt_remain, numbers = TRUE, sortVars = TRUE, labels = names(pt_remain), cex.axis = 0.7, gap = 3, ylab- c("Histogram of Missing Data", "Pattern"))
#mice()
  # impute numeric variable
  num.col = names(which(sapply(reg, is.numeric)))
  reg.num = reg[,num.col]
  if(sum(is.na(reg.num))!=0){
    reg.num.complete = complete(mice(reg.num, method = "cart", printFlag = F), 1)
    reg[,num.col] = reg.num.complete
  }
  reg = na.omit(reg)
  reg[,num.col]=scale(reg[,num.col])
  reg = reg[,colSums(is.na(reg))<nrow(reg)]
  
  # drop uni-value column again
  multi_value = sapply(reg,
                       function(x){
                         return(!length(unique(x[!is.na(x)])) == 1)})
  if(length(multi_value)!=0)
    reg = reg[, names(which(multi_value))]

```
separate data into train/test
```{r}
set.seed(1)
train.ind <- sample(1:dim(pt_remain)[1], dim(pt_remain)[1]*0.7)
train.data <- pt_remain[train.ind, ]
test.data <- pt_remain[-train.ind, ]
```

####
linear model
####Decision Tree
```{r}
formula <- paste("logerror ~ cnt_bathr + cnt_bedr + fips + type_landuse + id_rawcensus_block + region_county + cnt_room + tax_total + tax_land + tax_property + tax_building + sqrt_cal_area + year_built + region_city + sqrt_finished_liv + sqrt_lot")
# step 1 start with a small cp (complexity parameter, alpha)
tree0 <- rpart(formula, method = 'anova', data = train.data, 
               control=rpart.control(cp = 0.001))
# method = 'class' for classification
# method = 'anova' for regression
#complexity parameter smaller cp, more complex tree
tree0
# rel error = sum of squared over all leaf nodes of train data / sum of squared at root node
# xerror = sum of squared over all leaf nodes of left over data / sum of squared at root node 
printcp(tree0)
plotcp(tree0)
```
deviance: sse: sum((yi-ybar)^2): 
split in order to get max info_gain(entropy reduction)
first children: 2,3: sqrt_finished_liv, with * means it is leaf.
for i node, its parent is i/2, its left child is 2i, right child is 2i+1 (just like heap)
cp can control how complexity the tree is, the smaller cp is, the more complex the tree is.
```{r}
tree1 <- rpart(formula, method = "anova", data = train.data, control = rpart.control(cp=0.0001))
#tree1
printcp(tree1)
plotcp(tree1)
```
rel error means: sse.leaf/sse.noot, as nsplit increase, rel error decrease, because every split we designed to reduce sse. When every leaf contains only one observation, the sse.leaf=0. \\
xerror: sse.leaf/other cross validation set, we want to let sse be small for every cross validation dataset
\\cross validation
let R help us find the best cp, in which the xerror is smallest
```{r}
bestcp <- tree1$cptable[which.min(tree1$cptable[, "xerror"]), "CP"]
tree.pruned <- prune(tree1, cp=bestcp)
#because the root is smallest, ignore
cp.tab <- as.data.frame(tree1$cptable)
with(cp.tab, min(which(xerror - xstd < min(xerror))))
bestcp <- cp.tab$CP[with(cp.tab, min(which(xerror - xstd < min(xerror))))]
# Step 3: Prune the tree using the best cp.
tree.pruned <- prune(tree1, cp = bestcp)
tree.pruned

test.pred <- predict(tree.pruned, test.data)
sum((test.pred - test.data$logerror)^2) / length(test.pred) # 0.0246
# There could be error becuz test.data has new level for categorical variable, not found in train.data
# what should we do? impute it as NA
test.data$regionidzip[which(!test.data$regionidzip %in% train.data$regionidzip)] <- NA
test.pred <- predict(tree.pruned, test.data)
sum((test.pred - test.data$logerror)^2, na.rm = T) / length(which(!is.na(test.pred)))

# plot tree
plot(tree.pruned)
# no uniform, the length of branch indicates decrease of deviance
plot(tree.pruned, uniform = TRUE) 
# Since labels often extend outside the plot region it can be helpful to specify xpd = TRUE
text(tree.pruned, cex = 0.8, use.n = TRUE, xpd = TRUE)
# They can be quite ugly and hard to read, especially when you 
# have many levels for a factor since the plot automatically labels them using alphabets.

library(rpart.plot)
# http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html
# plot rpart model
# cex is the text size, faclen is the length of factor level names in splits.
prp(tree.pruned, faclen = 0, cex = 0.8)
# http://www.milbo.org/rpart-plot/prp.pdf
```
###Random Forest
categorical>>factor, as.formula, **no missing value**, need to impute
```{r}
train.data$fips <- as.factor(train.data$fips)
set.seed(2)
train.data <- train.data[which(apply(train.data, 1, function(x) length(which(is.na(x)))==0)), ]
length(is.na(train.data))
```

#Show underestimate region
```{r}
temp <- mutate(pt_remain, overunder = ifelse(logerror <0, "under",
                                  "over"))
library(leaflet)
leaflet()%>% addTiles()%>%
  fitBounds(-118.5,33.8,-118.25,34.15)%>%
  addRectangles(-118.5,33.8,-118.25,34.15)%>%
  addMiniMap()
```
